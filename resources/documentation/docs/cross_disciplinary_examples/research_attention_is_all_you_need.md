# Research Paper Example: Attention Is All You Need

**Authors:** Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin
**Published:** 2017
**Field:** Machine Learning, NLP

## Summary
This paper introduces the Transformer architecture, which relies entirely on attention mechanisms, dispensing with recurrence and convolutions. It revolutionized natural language processing and deep learning.

## Key Concepts
- Transformer architecture
- Attention mechanism
- Deep learning

## Machine-Readable Data
```json
{
  "title": "Attention Is All You Need",
  "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Łukasz Kaiser", "Illia Polosukhin"],
  "published": 2017,
  "field": ["machine learning", "NLP"],
  "keywords": ["transformer", "attention", "deep learning"]
}
```

## Applications
- Foundation for modern NLP models
- Deep learning research

## Cross-Links
- See `/knowledge/theories/` for more research papers
- See `/resources/documentation/docs/web/system_design/technology_comparison.md` for ML frameworks
